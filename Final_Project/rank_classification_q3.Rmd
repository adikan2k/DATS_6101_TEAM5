---
title: "Rank Classification"
author: "Annie Cheng"
date: "2024-12-08"
output: html_document
---

```{r include=FALSE}
library(ezids)
library(dplyr)
library(tidyverse)
library(ggplot2)
#install.packages("naniar")
library(naniar)
library(gridExtra)
```


# EDA

## Data Cleaning

### Understand the Dataset
```{r}
# Loading the dataset
top_songs <- read.csv('universal_top_spotify_songs.csv', na.strings = c("", "NA"))

# Print out the first 5 rows of the dataset
xkabledplyhead(top_songs,title="The first 5 rows of the dataset")
```

```{r include=FALSE}
str(top_songs)
```


```{r}
colnames(top_songs)
```

```{r}
summary(top_songs)
```


1. Drop columns which are not useful for modelling and have no meaning on the popularity of a song

2. The minimum value of time signature of a song is 0, which is not possible (Remove rows with time signature = 0).

3. According to the data description, NAs in `country` means Global Top 50. Therefore, we will fill NAs with 'Global'.

4. Since the percentage of observations with NAs is relatively low at 0.046% of the observations; therefore, we will drop all the other observations with NAs.

### Drop irrelevant columns

```{r}
remove_col <- c(1) # Add on columns
top_songs <- top_songs[, -remove_col]
```


### Handling Missing Values

Visualize the missing values in each column
```{r}
gg_miss_var(top_songs)
```


Calculate the counts and percentage of missing values in each columns
```{r}
# Counts the missing values in each column
colSums(is.na(top_songs))

# Calculate the percentage of missing values in each column
col_na_percentages <-round(colSums(is.na(top_songs))/dim(top_songs)[1]*100, 3)
col_na_percentages
```

```{r}
# Impute the null values in country with 'Global'
top_songs$country = ifelse(is.na(top_songs$country), 'Global', top_songs$country)

# Drop observations with missing values
df_cleaned <- na.omit(top_songs)

# Drop observations with 0 time signature
df_cleaned <- df_cleaned[df_cleaned$time_signature!=0, ]
```

```{r}
print("Dimension of Dataset before Cleaning")
print(dim(top_songs))
print("Dimension of Dataset after Cleaning")
dim(df_cleaned)
```

### Convert Variables into Correct Data Types

```{r include=FALSE}
str(df_cleaned)
```

```{r}
# Convert columns into date data type
df_cleaned$snapshot_date <- as.Date(df_cleaned$snapshot_date, format = "%Y-%m-%d")
df_cleaned$album_release_date <- as.Date(df_cleaned$album_release_date, format = "%Y-%m-%d")

# Convert columns into factor data type
df_cleaned$time_signature <-factor(df_cleaned$time_signature, level= c(1, 3, 4, 5))
df_cleaned$is_explicit <- factor(as.logical(df_cleaned$is_explicit))
df_cleaned$mode <- factor(df_cleaned$mode)

str(df_cleaned)
```

```{r}
summary(df_cleaned)
```

### Outliers Detection

```{r}
selected_columns <- c(
  "popularity", "duration_ms", "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo")

boxplots_list <- list()

# Create box plots for each selected column
for (col in selected_columns) {
  boxplot <- df_cleaned %>% 
    ggplot(aes(y = .data[[col]])) +
    geom_boxplot(fill = "lightblue") +
    labs(title = paste("Box Plot for", col), y = col) +
    theme_minimal()
  print(boxplot)
}
```



### Heatmap on Numeric Variables
#### Performance Metric
##### Popularity

```{r}

num_columns <- c(
  "popularity", "duration_ms", "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo", "daily_rank",
  "daily_movement", "weekly_movement")

corrplot::corrplot.mixed(cor(df_cleaned[,num_columns]))
```

###### Rank
```{r}
num_columns <- c(
  "daily_rank","popularity", "duration_ms", "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo", 
  "daily_movement", "weekly_movement")
corrplot::corrplot(cor(df_cleaned[,num_columns], method = "spearman"))
```



3. How does the explicitness of songs impact their popularity?

```{r}
colnames(df_cleaned)
```

```{r}
# See when does the data start recording top song ranks
# unique(df_cleaned$snapshot_date)
```

The snapshot date is from 2023-10-18 to 2024-11-20.


```{r}
df_cleaned %>% ggplot(aes(x = is_explicit, fill = is_explicit)) +
  geom_bar()
```
Most of songs do not contain explicit content.

# Bivariate Analysis
# popularity vs is_explicit
```{r}
explicit <- df_cleaned %>% filter(is_explicit == TRUE)
non_explicit <- df_cleaned %>% filter(is_explicit == FALSE)

# Histogram
df_cleaned %>% ggplot(aes(x=popularity, color=is_explicit)) +
  geom_histogram(fill = "white", alpha = 0.5)+
  scale_color_brewer(palette="Dark2")

# Boxplot
df_cleaned %>% ggplot(aes(x = is_explicit, y=popularity, fill=is_explicit)) +
  geom_boxplot()

print(t.test(explicit$popularity, non_explicit$popularity))
```

Songs with explicit content seems to have ahigher median and with higher value distribution too. After doing the t-test, we can reject the null hypothesis that the average popularity between songs with explicit content and without are significantly different.

Does specific type of songs more likely to contain explicit content?
```{r}

music_columns <- c(
  "duration_ms", "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo")

for (col in music_columns) {
  
  # histogram
  hist <- df_cleaned %>% ggplot(aes(x=.data[[col]], color=is_explicit)) +
  geom_histogram(fill = "white", alpha = 0.5)+
  scale_color_brewer(palette="Dark2")
  print(hist)

  # Boxplot
  boxplot <- df_cleaned %>% ggplot(aes(x = is_explicit, y=.data[[col]], fill=is_explicit)) +
  geom_boxplot()
  print(boxplot)
  
}

for (col in music_columns) {
  # t-test
  print(paste("T-test result for", col, ":"))
  print(t.test(explicit[[col]], non_explicit[[col]]))
}

```

Compare and contrast the histograms and box plots of musical features of songs containing explicit content or not, danceability and speechiness seems to differ significantly for songs contain explicit content and songs do not.

Based on t-test, we can see that valence with p-value = 0.4 fail to reject the null and liveness with p-values = 0.03 only slightly lower than the standard alpha 0.05. With all the other music features (`duration_ms`, `danceability`, `energy`, `loudness`, `speechiness`, `acousticness`, `instrumentalness`, `tempo`) have low p-value, we can reect the null hypothesis that the average of these music features are significantly different for explicit songs and non-explicit songs.



# 5. Can musical features determine a song ranked higher (top 25) or lower (25-50) on the global rank?


```{r}

# Prepare the Dataset
# Filter out only the global top 50 songs
global <- df_cleaned %>% filter(df_cleaned$country == "Global")

global <- global %>% mutate(
  top_25 =ifelse(global$daily_rank < 26, TRUE, FALSE),
  age = 
    as.numeric(format(global$snapshot_date, "%Y")) - as.numeric(format(global$album_release_date, "%Y")),
   duration_min = duration_ms/1000/60                                                
  )

str(global)

```


# EDA

## All Numerical Features
```{r}
library(plotly)

colnames(global)

selected_columns <- c( "daily_movement", "weekly_movement",
  "duration_min", "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo", "popularity", "age")

for (col in selected_columns) {
  
  # histogram
  hist <- global %>% ggplot(aes(x=.data[[col]], color=top_25)) +
  geom_histogram(fill = "white", alpha = 0.5)+
  scale_color_brewer(palette="Dark2")
  print(hist)

  # Boxplot
  boxplot <- global %>% ggplot(aes(x = top_25, y=.data[[col]], fill=top_25)) +
  geom_boxplot()
  print(ggplotly(boxplot))
}

top25 <- global %>% filter(top_25 == TRUE)
top50 <- global %>% filter(top_25 == FALSE)

for (col in selected_columns) {
  # t-test
  print(paste("T-test result for", col, ":"))
  print(t.test(top25[[col]], top50[[col]]))
}

```

## Musical Features

```{r}
music_columns <- c(
  "danceability", 
  "energy", "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "tempo")

# Select relevant columns for the pair plot
df_selected <- global[, num_columns]



loadPkg("psych") # pair plots with histogram on diagonal and other options
pairs.panels(df_selected, 
             method = "spearman", # correlation method
             hist.col = "#00AFBB",
             density = FALSE,  # show density plots
             ellipses = FALSE, # show correlation ellipses,
             main = "Pair Plot of Musical Features"
             )
unloadPkg("psych")

```



```{r fig.width=30, fig.height=20}

colnames(global)

selected_columns <- c("danceability", 
  "loudness", "speechiness", 
  "acousticness", "instrumentalness", "liveness",
  "valence", "energy", "tempo")

boxplot_list <- list()

for (col in selected_columns) {
  
  # Boxplot
  boxplot <- global %>% ggplot(aes(x = top_25, y = .data[[col]], fill = top_25)) +
    geom_boxplot() +
    theme(legend.position = "none")+
    labs(title = paste("Boxplot of", col))
  
  boxplot_list[[col]] <- boxplot
}

# Combine boxplots into grids
boxplot_grid <- subplot(boxplot_list, nrows = 3, margin = 0.05, titleX = TRUE, titleY = TRUE)

# Add a title to the grid
boxplot_grid <- boxplot_grid %>%
  layout(
    title = list(
      text = "Boxplot of Rank Groups vs Musical Features",
      font = list(size = 16, color = "black", family = "Arial"),
      x = 0.5,  # Center the title
      xanchor = "center"
    )
  )


boxplot_grid

```


According to t-test, the p-value of `tempo` and `energy` are not less than 0.05. There is no sufficient evidence to conclude that there is a significant difference in `tempo` and `energy` between the Top 25 and Top 26-50 groups. With all the p-values of other musical features (`loudness`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`,`valence`, `danceability`) less than 0.05, we can reject the null and conclude that there is a significant difference in these musical features (`loudness`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`,`valence`, `danceability`) between the Top 25 and Top 26-50 groups

Besides musical features, we also explore on other performance metrics, `daily_movement`, `weekly_movement` and `popularity`. __Daily and Weekly Movements__: Songs in the Top 25 show higher variability and more positive movement outliers, suggesting these songs may have larger shifts in rankings. __Popularity__: There is little difference in the popularity distributions between the Top 25 and Top 26-50 groups, implying that popularity might not be a strong differentiating factor for Top 25 songs. However, according to t-test, we can conclude that there is sufficient evidence to conclude that there is a significant difference in these musical featuers between the Top 25 and Top 26-50 groups.

Lastly, we will look at the `age` of song, which is defined as the year difference between the year the album released and the year it is ranked on the top 50 songs. We cannot observe much in the boxplot. The IQR for both groups is narrow and close to 0, suggesting that most songs are new.


# Modeling

## Logistic Regression

```{r}
loadPkg("leaps")

global$top_25 <- factor(global$top_25)

colnames(global)

global_cleaned <- global[, -c(1,2,3,6,7,10,11,12)]

str(global_cleaned)

# Put target variable `top_25` to the end of columns
global_cleaned <- global_cleaned[, c(setdiff(names(global_cleaned), "top_25"), "top_25")]

str(global_cleaned)

top25Logit0 = glm(top_25 ~1, data = global_cleaned, family = "binomial")
# Build model using all x varibles
top25Logit1 = glm(top_25 ~., data = global_cleaned, family = "binomial")

xkabledply(top25Logit1)
```

```{r include=FALSE}
# Backward Selection
step(top25Logit1)
# only excluding tempo
```


**Model (Backward Feature Selection)**
Call:  glm(formula = top_25 ~ daily_movement + weekly_movement + popularity + 
    is_explicit + duration_min + danceability + energy + key + 
    loudness + mode + speechiness + acousticness + instrumentalness + 
    liveness + valence + time_signature + age, family = "binomial", 
    data = global_cleaned)


## Forward Selection
```{r include=FALSE}

step(top25Logit0, scope = list(upper = top25Logit1, lower = top25Logit0), 
    method = "forward")

# Only excluding tempo
```

**Model (Forward Feature Selection)**

Call:  glm(formula = top_25 ~ weekly_movement + popularity + daily_movement + 
    danceability + key + speechiness + duration_ms + instrumentalness + 
    time_signature + acousticness + loudness + age + mode + liveness + 
    is_explicit + valence + energy, family = "binomial", data = global_cleaned)

    
    
```{r}

top25Logit_final = glm(
  formula = top_25 ~ weekly_movement + daily_movement + 
    danceability + key + speechiness + duration_min + instrumentalness + 
    acousticness + loudness + age + mode + liveness + 
    is_explicit + valence, family = "binomial", data = global_cleaned)


xkablevif(top25Logit_final)
```

After checking the Variance Inflation Factor (VIF), we identified the presence of multicollinearity in our model. To address this, we will remove variables with the highest VIF in a stepwise manner, starting with time_signature, energy, and popularity, to ensure all VIF values are below 10.


```{r}
# Model after removing variables > 10
top25Logit_final = glm(formula = top_25 ~ weekly_movement + daily_movement + 
    danceability + key + speechiness + duration_min + 
    instrumentalness + acousticness + loudness + age + mode + liveness + 
    is_explicit + valence, family = "binomial", data = global_cleaned)

summary(top25Logit_final)
```


```{r results='markup'}
xkabledply(top25Logit_final)
```

According to the summary of logistic regression, we will then drop the insignificant predictor `valence`.


```{r}

# Final Model
top25Logit_final = glm(
  formula = top_25 ~ weekly_movement + daily_movement + 
    danceability + key + speechiness + duration_min + instrumentalness + 
    acousticness + loudness + age + mode + liveness+
    is_explicit, family = "binomial", data = global_cleaned)

xkabledply(top25Logit_final)
summary(top25Logit_final)
```


```{r fig.width=40, fig.height=8}

# Exponentiate the estimated coefficients of logistic regression
expcoeff = exp(coef(top25Logit_final))
# expcoeff
xkabledply( as.table(expcoeff), title = "Exponential of coefficients in Logit Reg" )
```

__Musical Features__<br>
Notably, we can observe a strong positive relationship between __danceability__ and higher rankings.The odds of being in the Top 25 increase by approximately 750.5%. `key` and `mode1` all have positive impact association with higher rankings. While `speechiness`, `instrumentalness`, `acusticness`, `loudness` and `liveness` has negative association with higher rankings.

__Non Musical Features__<br>
`duration_min` has positive impact association with higher rankings.  Meanwhile, the exponential coefficient of `is_explicitTRUE` is less than 1 meaning that being an explicit song (TRUE) decreases the odds of being top 25 ranking by 11.38%.

__Performance Metric__<br>
Among performance metrics, both `weekly_movement` and `daily_movement` are positively associated with higher rankings, suggesting that better movement trends contribute to achieving higher chart positions.


### Final Model Performance ###
```{r, results='markup'}
loadPkg("caret")
loadPkg("pROC")
loadPkg("pscl")

SongLogit <-glm(
  formula = top_25 ~ weekly_movement + daily_movement + 
    danceability + key + speechiness + duration_min + instrumentalness + 
    acousticness + loudness + age + mode + liveness + 
    is_explicit, family = "binomial", data = global_cleaned)


summary(SongLogit)

# Convert fitted probabilities to predicted classes using a 0.5 threshold
global_cleaned$predicted <- as.factor(ifelse(SongLogit$fitted.values > 0.5, 1, 0))

# Ensure the reference variable (true labels) is a factor
global_cleaned$y <- as.factor(ifelse(global_cleaned$top_25==TRUE,1,0))


conf_matrix <- confusionMatrix(data = global_cleaned$predicted, 
                               reference = global_cleaned$y,
                               positive = "1")
print(conf_matrix)

prob = predict(SongLogit, type = "response")
h <- roc(global_cleaned$top_25~prob)
print(auc(h))
plot(h, main = "Logistic Regression ROC Curve", col = "blue", lwd = 3)
SongNullLogit <- glm(top_25 ~ 1, family = "binomial", data = global_cleaned)
mcFadden = 1 - logLik(SongLogit)/logLik(SongNullLogit)
mcFadden
pR2(SongLogit)

unloadPkg("caret")
unloadPkg("pROC")
unloadPkg("pscl")

colnames(global_cleaned)

# Drop `predicted` and `y` column
global_cleaned <- global_cleaned[, c(-20, -21)]
str(global_cleaned)
```

         
__Model Evaluation on Logistic Regression__

The logistic regression model demonstrates moderate accuracy at 61.4%, with stronger sensitivity (64.6%) compared to weaker specificity (58.1%). This imbalance suggests that the model is biased towards predicting class 0.

The AUC is 0.677, which falls below the threshold of 0.8, indicating a relatively low ability to distinguish between classes. Additionally, the McFadden’s R² value is only 0.0832, meaning the model explains just 8.32% of the variation in the target variable. These metrics suggest that the model does not provide a good fit. Therefore, exploring alternative modeling techniques is recommended.


## KNN

```{r}
# Prepare dataframe for KNN
df_knn <- global_cleaned

# Put target variable `top_25` to the end of columns
df_knn <- df_knn[, c(setdiff(names(df_knn), "top_25"), "top_25")]


# convert all X variables into numeric data types
df_knn$is_explicit = as.numeric(df_knn$is_explicit)
df_knn$mode = as.numeric(df_knn$mode)
df_knn$time_signature = as.numeric(df_knn$time_signature)

str(df_knn)
str(df_knn[c(-19)])
```


### Full Model

```{r}

scaledglobal <- as.data.frame(scale(df_knn[c(-19)], center = TRUE, scale = TRUE))
set.seed(321)
global_sample <- sample(2, nrow(scaledglobal), replace=TRUE, prob=c(0.75, 0.25))

train_X <- scaledglobal[global_sample==1, 1:18]
test_X <- scaledglobal[global_sample==2, 1:18]

train_y <- df_knn[global_sample==1, 19]
test_y <- df_knn[global_sample==2, 19]
```

#### Tuning: Test set

```{r}
loadPkg("FNN")
loadPkg("gmodels") # CrossTable
loadPkg("caret") # confusionMatrix
ResultDf = NULL

unique(test_y)

for (kval in 1:11) {
  rank_pred <- knn(train = train_X, test = test_X, cl=train_y, k=kval)
  RankCross <- CrossTable(test_y, rank_pred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  RankCross
  # 
  cm = confusionMatrix(rank_pred, reference = test_y, positive = "TRUE" ) # from caret library
  # print.confusionMatrix(cm)
  # 
  cmaccu = cm$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu ) )
  # print("Other metrics : ")
  # print(cm$byClass)
  # 
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  # cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}

unloadPkg("FNN")
unloadPkg("gmodels") 
unloadPkg("caret")

```

```{r, results="asis"}
xkabledply(ResultDf, "Total Accuracy Summary")
```



```{r}
loadPkg("gmodels")
loadPkg("FNN")
# re-do that one with max accuracy
rank_pred_m <- knn(train = train_X, test = test_X, cl=train_y, k=3)
RankCross_m <- CrossTable(test_y, rank_pred_m, prop.chisq = FALSE)
unloadPkg("FNN")
unloadPkg("gmodels")
```

```{r}
loadPkg("caret") 
cmknn = confusionMatrix(rank_pred_m, reference = test_y, positive = "TRUE")
cmknn
print('Overall: ')
cmknn$overall
print('Class: ')
cmknn$byClass
unloadPkg("caret")
```



#### Tuning: Cross Validation

```{r}
loadPkg("caret")

# Tuning: Finding the best k
knnModel <- train(
  x = train_X, 
  y = train_y, 
  method = "knn", 
  trControl = trainControl(method = "cv"), 
  tuneGrid = data.frame(k = c(3, 5, 7, 9, 11, 13))
)


best_model<- knn3(
                  x = train_X,
                  y = train_y,
                  k = knnModel$bestTune$k
                 )

best_model

# Model Evaluation
predictions <- predict(best_model, test_X,type = "class")

# Calculate confusion matrix
cm <- confusionMatrix(predictions, test_y, positive = "TRUE")
cm
unloadPkg("caret")
```


#### Final Model Evaluation
```{r}
loadPkg("pROC")
loadPkg("FNN")


# Generate probabilities for KNN
probabilities <- knn(train = train_X, test = test_X, cl = train_y, k = 3, prob = TRUE)
prob_positive <- attr(probabilities, "prob")
prob_positive <- ifelse(probabilities == "TRUE", prob_positive, 1 - prob_positive)

# Calculate and plot ROC curve
roc_curve <- roc(test_y, prob_positive)
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
plot(roc_curve, main = "KNN ROC Curve (Full Model)", col = "blue", lwd = 2)
```

__Model Evaluation on KNN (Full Model)__<br>

We started by building a model using all features and employed both a train-test split and cross-validation to tune and validate the performance. For the test set, k=2, achieved the highest accuracy (0.850). However, to balance high accuracy with the risk of overfitting, k=3, achieved the highest accuracy (0.850). However, to balance high accuracy with the risk of overfitting, k=3, consistently delivered the best performance across all metrics, including total accuracy, specificity, and sensitivity, each averaging around 85%. The model demonstrates balanced performance across both classes, with sensitivity, specificity, and precision all near 85%. Additionally, the AUC score of 0.913 (above the 0.8 threshold) confirms that the model is a strong fit for the data.


### Musical Characteristics

Now we only select musical characteristics (`danceability`,`energy`, `key`, `loudness`, `mode`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, `time_signature`)

```{r}

colnames(df_knn)

# Select only Musical Characteristics
scaledglobal_music <- as.data.frame(scale(df_knn[,5:16], center = TRUE, scale = TRUE))

scaledglobal_music

set.seed(321)
global_sample_music <- sample(2, nrow(scaledglobal_music), replace=TRUE, prob=c(0.75, 0.25))

train_X <- scaledglobal_music[global_sample_music==1, ]
test_X <- scaledglobal_music[global_sample_music==2, ]

train_y <- df_knn[global_sample_music==1, 19]
test_y <- df_knn[global_sample_music==2, 19]

```

#### Tuning: Test Set

```{r}
loadPkg("FNN")
loadPkg("gmodels") # CrossTable
loadPkg("caret") # confusionMatrix
ResultDf = NULL
for (kval in 1:15) {
  rank_pred <- knn(train = train_X, test = test_X, cl=train_y, k=kval)
  RankCross <- CrossTable(test_y, rank_pred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  RankCross
  # 
  cm = confusionMatrix(rank_pred, reference = test_y, positive = "TRUE") # from caret library
  # print.confusionMatrix(cm)
  # 
  cmaccu = cm$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu ) )
  # print("Other metrics : ")
  # print(cm$byClass)
  # 
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  # cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}

```

```{r, results="asis"}
xkabledply(ResultDf, "Total Accuracy Summary")
```


#### Tuning: Cross Validation
```{r}
loadPkg("caret")
knnModel <- train(
  x = train_X, 
  y = train_y, 
  method = "knn", 
  trControl = trainControl(method = "cv"), 
  tuneGrid = data.frame(k = c(3, 5, 7, 9, 11, 13))
)

knnModel

best_model<- knn3(
                  x = train_X,
                  y = train_y,
                  k = knnModel$bestTune$k
                 )

# Model Evaluation
predictions <- predict(best_model, test_X,type = "class")
# Calculate confusion matrix
cm <- confusionMatrix(predictions, test_y, positive = "TRUE")
cm

```


```{r}
loadPkg("FNN")
loadPkg("gmodels")
# re-do that one with max accuracy
rank_pred_m <- knn(train = train_X, test = test_X, cl=train_y, k=7)
RankCross_m <- CrossTable(test_y, rank_pred_m, prop.chisq = FALSE)
unloadPkg("FNN")
unloadPkg("gmodels")
```

```{r}
loadPkg("caret") 
cmknn = confusionMatrix(rank_pred_m, reference = test_y, positive = "TRUE")
cmknn
print('Overall: ')
cmknn$overall
print('Class: ')
cmknn$byClass
unloadPkg("caret")
```

```{r}
library(pROC)
loadPkg("FNN")

# Generate probabilities for KNN
probabilities <- knn(train = train_X, test = test_X, cl = train_y, k = 7, prob = TRUE)
prob_positive <- attr(probabilities, "prob")
prob_positive <- ifelse(probabilities == "TRUE", prob_positive, 1 - prob_positive)

# Calculate and plot ROC curve
roc_curve <- roc(test_y, prob_positive)
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
plot(roc_curve, main = "KNN ROC Curve (Musical Characteristics)", col = "blue", lwd = 2)

```

__Model Evaluation on KNN (Musical Features)__<br>

For the test set, k= 7, yield the highest accuracy (0.747). However, cross-validation identified k=3 as the best model, with an accuracy of 75.3% and balanced sensitivity (72.6%) and specificity (77.9%). While k=3 performed slightly better at identifying class 1 than class 0, it maintained a good overall balance in cross-validation.

When comparing the two approaches, we observed different best values for k. Using k=3 n the test set resulted in a larger discrepancy between specificity and sensitivity, which could indicate reduced robustness. Meanwhile, the cross-validation performance for k=7 demonstrated consistent accuracy (around 0.7472), aligning closely with the test set performance for k=7.

Given this consistency and the stability of metrics, we selected k=7 as the best model. The AUC score of 0.8136 (greater than 0.8) further confirms the model’s effectiveness and suitability.



__Key Findings and Insights__

Logistic regression reveals that people prefer energetic and danceable music, while niche qualities like live recordings or high speech content have less universal appeal. Comparing k-NN models with all features versus only musical features shows that while musical features are strong predictors, incorporating non-musical factors (e.g., explicit content, duration, age) and performance metrics (e.g., weekly and daily movement) enhances prediction. Overall, k-NN outperforms logistic regression, likely due to its ability to capture non-linear relationships between musical features and rank groups, as observed in the EDA.

